# -*- coding: utf-8 -*-
"""Practice_18_RS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PGjoIF_dpPWjK-yGaRA0WAPoVWizH3tX
"""

import pandas as pd

ratings = pd.read_csv("ratings.csv")
movies = pd.read_csv("movies.csv")



import warnings
warnings.filterwarnings('ignore')

import matplotlib as mlp

mlp.rcParams['lines.linewidth'] = 5

mlp.rcParams['xtick.major.size'] = 20
mlp.rcParams['xtick.major.width'] = 5
mlp.rcParams['xtick.labelsize'] = 20
mlp.rcParams['xtick.color'] = '#FF5533'

mlp.rcParams['ytick.major.size'] = 20
mlp.rcParams['ytick.major.width'] = 5
mlp.rcParams['ytick.labelsize'] = 20
mlp.rcParams['ytick.color'] = '#FF5533'

mlp.rcParams['axes.labelsize'] = 20
mlp.rcParams['axes.titlesize'] = 20
mlp.rcParams['axes.titlecolor'] = '#00B050'
mlp.rcParams['axes.labelcolor'] = '#00B050'


import numpy as np
import matplotlib.pyplot as plt
import seaborn as snspip

ratings = ratings.sort_values("timestamp")

train = ratings.iloc[:-20000].copy()
test = ratings.iloc[-20000:].copy()

train.sort_values("timestamp")

test.sort_values("timestamp")

### User-based approach

pivot = train.pivot_table(index='movieId',
                          columns='userId',
                          values='rating')

corrs = pivot.corr()

corrs

corrs = (
    corrs
    .stack()
    .rename_axis(['userId1', 'userId2'])
    .reset_index()
)

corrs.columns = ['userId1', 'userId2', 'corr']

corrs = corrs[corrs['corr'] >= 0]

corrs

import math

preds = []  # will store predictions for all users

# Iterate over each unique user in the test set
for user in test['userId'].unique():

    ### Step 1. Check if the user exists in the training data
    ### If the user is completely new (a "cold start" case),
    ### we cannot use neighbor-based collaborative filtering,
    ### because we have no information about this user’s past ratings.
    if user in train['userId'].unique():
        part = test[test['userId']==user]

        ### Step 2. Find neighbors of the current user
        ### "Neighbors" are other users who have some similarity (correlation) with this one.
        neighbours = corrs[corrs['userId1']==user]
        neighbours_users = neighbours['userId2'].unique()

        ### Step 3. Handle the case of no neighbors
        ### If no similar users exist, we cannot use neighborhood-based prediction.
        ### In practice, we might fall back to a baseline (like global mean),
        ### but here we just skip this user.
        if neighbours_users.shape[0] == 0:
            continue

        ### Step 4. Identify which movies we need to predict for this user
        movies_ = part['movieId'].unique()

        ### Step 5. Get training ratings only from the neighbors
        ### These ratings will help us estimate predictions for the target user.
        train_part = train[train['userId'].isin(neighbours_users)]

        ### Step 6. Compute each neighbor’s average rating
        ### This helps us normalize ratings: some users give consistently higher/lower ratings.
        neighbours_means = train_part.groupby('userId')['rating'].mean()

        ### Step 7. Keep only neighbor ratings on the movies we want to predict
        ### Then, join with correlation values (similarity between target user and neighbor).
        train_part = train_part[train_part['movieId'].isin(movies_)]
        train_part = pd.merge(
            train_part,
            neighbours[['userId2', 'corr']],  # similarity values
            right_on='userId2',
            left_on='userId',
            how='left'
        )

        ### Step 8. Adjust neighbor ratings relative to their average
        ### - "neighbour_mean" = each neighbor’s mean rating
        ### - "diff" = rating - mean (centered rating)
        ### - "diff_dot_corr" = weighted by similarity with the target user
        train_part['neighbour_mean'] = train_part['userId2'].map(neighbours_means)
        train_part['diff'] = train_part['rating'] - train_part['neighbour_mean']
        train_part['diff_dot_corr'] = train_part['diff'] * train_part['corr']

        ### Step 9. Compute the mean rating of the target user
        ### This serves as the baseline to which we add adjustments from neighbors.
        user_mean = train[train['userId']==user]['rating'].mean()

        ### Step 10. Apply the prediction formula
        ### Formula:
        ###   pred(u, i) = user_mean(u) +
        ###                sum_over_neighbors( (rating(v,i) - mean(v)) * sim(u,v) ) /
        ###                sum_over_neighbors( sim(u,v) )
        ###
        ### Numerator = weighted sum of deviations from neighbor means
        ### Denominator = sum of similarities
        upper_part = train_part.groupby('movieId')['diff_dot_corr'].sum()
        lower_part = train_part.groupby('movieId')['corr'].sum()

        predictions = upper_part / lower_part + user_mean

        # Clean up the result and add userId column
        predictions = predictions.reset_index()
        predictions.columns = ['movieId', 'prediction']
        predictions['userId'] = user

        # Save predictions for this user
        preds.append(predictions)

# Concatenate predictions from all users
preds = pd.concat(preds)

# Merge predictions with the test set so we can later evaluate accuracy
preds = pd.merge(
    preds,
    test[['userId', 'movieId', 'rating']],
    on=['userId', 'movieId'],
    how='left'
)

print(f"""We were able to make predictions only for {preds.shape[0]}
          user–item pairs out of {test.shape[0]} in the test set""")

preds

### Calculate DSG@2 (a simplified ranking quality metric) for these pairs!
### DSG@k here means: we take the top-k recommendations for each user,
### weight them by their relevance (true rating), and discount them by position.
### The idea: highly ranked relevant items should contribute more than lower-ranked ones.

users_dsgs = []

for user in preds['userId'].unique():
    # Select predictions for the current user
    part = preds[preds['userId']==user]

    # Sort movies by predicted rating (model’s ranking, descending)
    part = part.sort_values('prediction', ascending=False)
    part = part.reset_index()

    # Compute DSG@2:
    # - "part.index+1" → position of the item in the ranked list (starting from 1)
    # - "np.log2(position)" → discount factor: items further down the list count less
    # - "part.rating" → the true relevance signal (actual rating from test set)
    # - Take only the top 2 items ([:2]) and sum them up
    user_dsg2 = (np.log2(part.index+1) * part.rating)[:2].sum()

    users_dsgs.append(user_dsg2)

# Finally, compute the average DSG@2 across all users in the test set
print(f"Average DSG@2 across users from the test set: {np.mean(users_dsgs)}")

"""### Content approach"""

df = pd.merge(
    ratings,
    movies,
    on='movieId',
    how='left'
)

df.head()

### Every movie has genre and title

df.isna().sum()

### Extract year from titles

import re

def find_num(st):

    nums_list = re.findall(r'\d+', st)

    if len(nums_list) > 0:
        return nums_list[-1]
    else:
        return '0'

def filter_missing_data(num):
    if num > 1900:
        return num
    else:
        return 2000

df['movieYear'] = df['title'].apply(lambda x: filter_missing_data(int(find_num(x))))

df.head()

### One-Hot Encoding of genres!

all_genres = ['Adventure', 'Comedy', 'Action', 'Mystery', 'Crime', 'Thriller',
              'Drama', 'Animation', 'Children', 'Horror', 'Documentary',
              'Sci-Fi', 'Fantasy', 'Film-Noir', 'Western', 'Musical', 'Romance',
              '(no genres listed)', 'War']

for genre in all_genres:
    df[genre] = (
        df['genres']
        .str
        .contains(genre)
        .apply(int)
    )

df = df.drop('genres', axis=1)

df.head()

### Train Test split

train_new = df.iloc[:-20000].copy()
test_new = df.iloc[-20000:].copy()

train_new

### Let’s add at least some extra information about users!
### For example:
### - How many movies they have watched (in the training period)
### - What their average rating is (with a little added noise for realism)

# Count how many ratings (movies watched) each user has in the training set
user_count_views = train_new.groupby('userId').size()

# Compute each user’s mean rating
user_means = train_new.groupby('userId')['rating'].mean()

# Add the number of movies each user has rated to the dataset
train_new['userViews'] = train_new['userId'].map(user_count_views)

# Add user mean ratings with some Gaussian noise (std = 0.1)
# Noise is added so that features look less “perfect” and more realistic,
# simulating the fact that user behavior is not always stable.
noise = np.random.normal(0, 0.1, [train_new.shape[0],])
train_new['userMeans'] = train_new['userId'].map(user_means) + noise

### Compute global averages to handle missing users
### - overall_views_mean: average number of movies watched across all users
### - overall_meanrating_mean: average of users’ mean ratings
### These will be used as fallback values for users not seen in training (cold start).

overall_views_mean = int(user_count_views.mean())
overall_meanrating_mean = int(user_means.mean())

# Add user-level features to the test set
# If a test user was not present in the training set,
# we replace missing values with the global averages calculated above.
test_new['userViews'] = (
    test_new['userId']
    .map(user_count_views)       # how many movies the user watched
    .fillna(overall_views_mean)  # fallback: global average
)

test_new['userMeans'] = (
    test_new['userId']
    .map(user_means)                 # average rating given by the user
    .fillna(overall_meanrating_mean) # fallback: global average
)

# Drop identifiers and metadata that we don’t want to use as features
# These columns are not useful for training predictive models directly
train_new = train_new.drop(['userId', 'movieId',
                            'timestamp', 'title'], axis=1)

test_new = test_new.drop(['userId', 'movieId',
                          'timestamp', 'title'], axis=1)

train_new

X_train = train_new.drop('rating', axis=1)
X_test = test_new.drop('rating', axis=1)

y_train = train_new['rating']
y_test = test_new['rating']

from catboost import CatBoostRegressor, Pool

catboost = CatBoostRegressor()


catboost.fit(X_train,
             y_train,
             cat_features=['movieYear'],
             )

# Commented out IPython magic to ensure Python compatibility.
# %pip install catboost

test_new = df.iloc[-20000:].copy()

X_test['pred'] = catboost.predict(X_test)
X_test['target'] = y_test
X_test['userId'] = test_new['userId']
X_test['movieId'] = test_new['movieId']

users_dsgs = []

# Iterate over each unique user in the test set
for user in X_test['userId'].unique():
    # Select predictions for the current user
    part = X_test[X_test['userId'] == user]

    # Sort items (movies) by predicted score (ascending here –
    # usually we’d expect descending if higher = better prediction)
    part = part.sort_values('pred')
    part = part.reset_index()

    # Compute DSG@2 for this user:
    # - "part.index+1" → position of the item in the ranked list
    # - "np.log2(position)" → discount factor (items lower in the list get less weight)
    # - "part.target" → true relevance label (e.g., rating or binary relevance)
    # - [:2] → consider only the top-2 ranked items
    # - sum() → aggregate the discounted relevance of top-2 items
    user_dsg2 = (np.log2(part.index+1) * part.target)[:2].sum()

    users_dsgs.append(user_dsg2)

# Compute the average DSG@2 across all users in the test set
print(f"Average DSG@2 across test users: {np.mean(users_dsgs)}")

preds

# Merge the predictions with X_test to include model scores ("pred")
# This keeps only user–movie pairs that exist in both datasets.
new_preds = pd.merge(
    preds,
    X_test[['userId', 'movieId', 'pred']],
    on=['userId', 'movieId'],
    how='left'
)

users_dsgs = []

# Iterate over each user for whom we have predictions
for user in new_preds['userId'].unique():
    # Select all movie predictions for this user
    part = new_preds[new_preds['userId'] == user]

    # Sort movies by model-predicted score ("pred")
    # Note: sorted ascending here – usually ranking metrics sort descending
    part = part.sort_values('pred')
    part = part.reset_index()

    # Compute DSG@2 for this user:
    # - (part.index + 1): the rank position of each item
    # - np.log2(rank): discount factor → lower-ranked items contribute less
    # - part.rating: true rating from the test set (relevance signal)
    # - [:2].sum(): consider only top-2 items and sum their discounted relevance
    user_dsg2 = (np.log2(part.index+1) * part.rating)[:2].sum()

    users_dsgs.append(user_dsg2)

# Report the average DSG@2 across all test users who were also in training
print(f"Average DSG@2 for test users that were present in training: {np.mean(users_dsgs)}")




from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
import pickle

# datasetləri oxu
ratings = pd.read_csv("ratings.csv")
movies = pd.read_csv("movies.csv")

# join et
data = ratings.merge(movies, on="movieId")

# sadə feature seçimi (userId, movieId ilə başlayaq)
X = data[["userId", "movieId"]]
y = data["rating"]

# train-test böl
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# CatBoost model
model = CatBoostRegressor(verbose=100, iterations=300, learning_rate=0.1, depth=6)
model.fit(X_train, y_train, eval_set=(X_test, y_test))

# practice_18_rs.py dosyasının EN SONUNA ekle:

# Modeli kaydet
import pickle

with open('movie_rating_model.pkl', 'wb') as f:
    pickle.dump(model, f)

# Film listesini kaydet
movies.to_csv('movies_for_web.csv', index=False)

print("✅ Model kaydedildi: movie_rating_model.pkl")
print("✅ Film listesi kaydedildi: movies_for_web.csv")